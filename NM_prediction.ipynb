{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will try and predict one NM from another one using the low dimensional embeddings from CEBRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/cebra/datasets/__init__.py:103: UserWarning: Could not initialize one or more datasets: No module named 'h5py'. For using the datasets, consider installing the [datasets] extension via pip.\n",
      "  warnings.warn(f\"Could not initialize one or more datasets: {e}. \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os # my addtion\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.integrate import solve_ivp\n",
    "import cebra.data\n",
    "import torch\n",
    "import cebra.integrations\n",
    "import cebra.datasets\n",
    "from cebra import CEBRA\n",
    "import torch\n",
    "import pickle\n",
    "import cebra_pack.utils as cp\n",
    "import sklearn.linear_model\n",
    "import logging\n",
    "import glob\n",
    "\n",
    "\n",
    "from matplotlib.collections import LineCollection\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Load the Data\n",
    "\n",
    "Here we load data from the Fibre Photometry pipeline of 4 Neuromodulators (DA, 5HT, ACh, NE) recorded in the Nucleus Acumbens region. The main neural data will be in the form of dF_F traces of these 4 Neuromodulators (NMs). These will be stored in a 2D array, 'all_nms'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pandas==2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df.pkl from the CO path.\n"
     ]
    }
   ],
   "source": [
    "# load the dataframe that contains data from 1 session\n",
    "try:\n",
    "    df_trials_ses = pickle.load(open('/Users/brian.gitahi/Desktop/AIND/CEBRA/Git/CEBRA-Demo/data/CO data/df.pkl', \"rb\"))\n",
    "    print(\"Loaded df.pkl from the local path.\")\n",
    "except FileNotFoundError:\n",
    "    df_trials_ses = pickle.load(open('/data/df.pkl', \"rb\"))\n",
    "    print(\"Loaded df.pkl from the CO path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded traces.pkl from the alternate path.\n"
     ]
    }
   ],
   "source": [
    "# download the dictionary containing the traces (you can download this locally and replcace the path)\n",
    "try:\n",
    "    traces = pickle.load(open('/Users/brian.gitahi/Desktop/AIND/CEBRA/Git/CEBRA-Demo/data/CO data/traces.pkl', \"rb\"))\n",
    "    print(\"Loaded traces.pkl from the local path.\")\n",
    "except FileNotFoundError:\n",
    "    traces = pickle.load(open('/data/traces.pkl', \"rb\"))\n",
    "    print(\"Loaded traces.pkl from the alternate path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Trace times.npy from the alternate path.\n"
     ]
    }
   ],
   "source": [
    "# load the trace times (you can download this locally and replcace the path)\n",
    "try:\n",
    "    trace_times = np.load('/Users/brian.gitahi/Desktop/AIND/CEBRA/Git/CEBRA-Demo/data/CO data/Trace times.npy', allow_pickle=True)\n",
    "    print(\"Loaded Trace times.npy from the local path.\")\n",
    "except FileNotFoundError:\n",
    "    trace_times = np.load('/data/Trace times.npy', allow_pickle=True)\n",
    "    print(\"Loaded Trace times.npy from the alternate path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the choice time \n",
    "choice_times = df_trials_ses['choice_time'][0:n_trials].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218572, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine the traces into one 2D array\n",
    "all_nms = np.array([traces[trace] for trace in traces.keys()])\n",
    "all_nms = np.transpose(all_nms)\n",
    "all_nms.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Format the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural tensor shape:  torch.Size([1717, 80])\n",
      "reward labels shape:  (1717,)\n",
      "choice labels shape:  (1717,)\n",
      "rpe labels shape: (1717,)\n"
     ]
    }
   ],
   "source": [
    "# get all NMs\n",
    "formatted_nms, reward_labels, choice_labels, rpe_labels_ = cp.format_data(all_nms,df=df_trials_ses,trace_times_=trace_times, choice_times_=choice_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of formatted array: (218572, 1)\n",
      "shape of formatted array: (218572, 1)\n",
      "shape of formatted array: (218572, 1)\n",
      "shape of formatted array: (218572, 1)\n"
     ]
    }
   ],
   "source": [
    "# get individual NMs\n",
    "ind_nms = cp.individual_datasets(traces_=traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural tensor shape:  torch.Size([1717, 20])\n",
      "reward labels shape:  (1717,)\n",
      "choice labels shape:  (1717,)\n",
      "rpe labels shape: (1717,)\n",
      "neural tensor shape:  torch.Size([1717, 20])\n",
      "reward labels shape:  (1717,)\n",
      "choice labels shape:  (1717,)\n",
      "rpe labels shape: (1717,)\n",
      "neural tensor shape:  torch.Size([1717, 20])\n",
      "reward labels shape:  (1717,)\n",
      "choice labels shape:  (1717,)\n",
      "rpe labels shape: (1717,)\n",
      "neural tensor shape:  torch.Size([1717, 20])\n",
      "reward labels shape:  (1717,)\n",
      "choice labels shape:  (1717,)\n",
      "rpe labels shape: (1717,)\n"
     ]
    }
   ],
   "source": [
    "formatted_ind_nms = [cp.format_data(ind_nm, df_trials_ses, trace_times, choice_times)[0] for ind_nm in ind_nms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewarded, unrewarded = cp.define_label_classes(reward_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1717"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_nms.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add zeros to the individual nms data so the input dimensionality is the same as all_nms\n",
    "zeros = np.zeros((formatted_nms.shape[0],60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of zeros, add nans and compare with what we get\n",
    "nans = np.full([formatted_nms.shape[0],60], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select zeros or nans\n",
    "filler = zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_ind_nms = [torch.from_numpy(np.hstack((ind_nm, filler))) for ind_nm in formatted_ind_nms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0080,  0.0014, -0.0008,  ...,  0.0019,  0.0020,  0.0025],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_ind_nms[3][:,18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1717, 20])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_ind_nms[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Build and train the CEBRA model then compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train CEBRA on all nms then produce embeddings for individual nms\n",
    "def multi_embed_NMs(all_nms, ind_nms, b_label, max_iterations=2000, d=3, metric='euclidean', arch = 'offset10-model-mse'):\n",
    "\n",
    "\n",
    "    # build time and behaviour models\n",
    "    cebra_time_model = CEBRA(model_architecture=arch,\n",
    "                        batch_size=512,\n",
    "                        learning_rate=3e-4,\n",
    "                        temperature=1,\n",
    "                        output_dimension=d,\n",
    "                        max_iterations=max_iterations,\n",
    "                        distance=metric,\n",
    "                        conditional='time',\n",
    "                        device='cuda_if_available',\n",
    "                        verbose=True,\n",
    "                        time_offsets=10) \n",
    "\n",
    "    cebra_behaviour_model = CEBRA(model_architecture=arch,\n",
    "                        batch_size=512,\n",
    "                        learning_rate=3e-4,\n",
    "                        temperature=1,\n",
    "                        output_dimension=d,\n",
    "                        max_iterations=max_iterations,\n",
    "                        distance=metric,\n",
    "                        conditional='time_delta',\n",
    "                        device='cuda_if_available',\n",
    "                        verbose=True,\n",
    "                        time_offsets=10)\n",
    "\n",
    "    # train them both\n",
    "    cebra_time_model.fit(all_nms)\n",
    "    cebra_behaviour_model.fit(all_nms, b_label)\n",
    "\n",
    "    # compute the embeddings for the individual nms\n",
    "    time_embeddings = [cebra_time_model.transform(ind_nm) for ind_nm in ind_nms]\n",
    "    behaviour_embeddings = [cebra_behaviour_model.transform(ind_nm) for ind_nm in ind_nms]\n",
    "\n",
    "    # comput embeddings for all nms\n",
    "    time_embeddings.append(cebra_time_model.transform(all_nms))\n",
    "    behaviour_embeddings.append(cebra_behaviour_model.transform(all_nms))\n",
    "\n",
    "\n",
    "    return time_embeddings, behaviour_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pos:  0.9112 neg:  2.8213 total:  3.7326 temperature:  1.0000: 100%|██████████| 2000/2000 [00:19<00:00, 101.01it/s]\n",
      "pos:  0.2814 neg:  5.4531 total:  5.7345 temperature:  1.0000: 100%|██████████| 2000/2000 [00:18<00:00, 107.03it/s]\n"
     ]
    }
   ],
   "source": [
    "time_embeddings, behaviour_embeddings = multi_embed_NMs(all_nms=formatted_nms, ind_nms=expanded_ind_nms,b_label=reward_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. View Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's view the embeddings and compare them with those generated from independent CEBRA models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to view the embeddings\n",
    "def view(time_embedding, behaviour_embedding, labels, label_classes, scores=None, titles=[\"Time embedding\", \"Behaviour embedding\"], size=5):\n",
    " \n",
    "    # create a figure and make the plots\n",
    "    fig = plt.figure(figsize=(17,8))\n",
    "    gs = gridspec.GridSpec(1, 2, figure=fig)\n",
    "\n",
    "\n",
    "    ax81 = fig.add_subplot(gs[0,0], projection='3d')\n",
    "    ax82 = fig.add_subplot(gs[0,1], projection='3d')\n",
    "    \n",
    "    for ax in [ax81,ax82]:\n",
    "        ax.set_xlabel(\"latent 1\", labelpad=0.001, fontsize=13)\n",
    "        ax.set_ylabel(\"latent 2\", labelpad=0.001, fontsize=13)\n",
    "        ax.set_zlabel(\"latent 3\", labelpad=0.001, fontsize=13)\n",
    "\n",
    "        # Hide X and Y axes label marks\n",
    "        ax.xaxis.set_tick_params(labelbottom=False)\n",
    "        ax.yaxis.set_tick_params(labelleft=False)\n",
    "        ax.zaxis.set_tick_params(labelright=False)\n",
    "\n",
    "        # Hide X and Y axes tick marks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "\n",
    "\n",
    "    # colour maps\n",
    "    colours = ['cool', 'plasma', 'spring']\n",
    "\n",
    "    embeddings = [time_embedding, behaviour_embedding]\n",
    "\n",
    "    if scores == None:\n",
    "        scores, errors = np.round(cp.get_auc(embeddings, labels),3)\n",
    "\n",
    "    # plot the time embedding \n",
    "    cebra.plot_embedding(embedding=time_embedding[label_classes[0],:], embedding_labels=labels[label_classes[0]],ax=ax81, markersize=size, title=titles[0], cmap=colours[0])\n",
    "    cebra.plot_embedding(embedding=time_embedding[label_classes[1],:], embedding_labels=labels[label_classes[1]],ax=ax81, markersize=size, title= f'{titles[0]}, Score:{scores[0]}', cmap=colours[1])\n",
    "\n",
    "\n",
    "    # plot the behaviour embedding \n",
    "    cebra.plot_embedding(embedding=behaviour_embedding[label_classes[0],:], embedding_labels=labels[label_classes[0]],ax=ax82, markersize=size, title=titles[1], cmap=colours[0],)\n",
    "    cebra.plot_embedding(embedding=behaviour_embedding[label_classes[1],:], embedding_labels=labels[label_classes[1]],ax=ax82,markersize=size, title= f'{titles[1]}, Score: {scores[1]}',  cmap=colours[1])\n",
    "\n",
    "    gs.tight_layout(figure=fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first make function to make the plots given a list of embeddings\n",
    "def plot4_embeddings(embeddings, labels , l_class, t=\"\"):\n",
    "\n",
    "    # number of plots\n",
    "    n_plots = len(embeddings)\n",
    "\n",
    "    n_columns = 2\n",
    "    n_rows = (n_plots + 1) // n_columns\n",
    "\n",
    "    # create axis\n",
    "    fig = plt.figure(figsize=(16, 4*n_rows))\n",
    "    gs = gridspec.GridSpec(n_rows, n_columns, figure=fig)\n",
    "\n",
    "    # colour \n",
    "    c = ['cool','plasma','pink','winter']\n",
    "\n",
    "    for i, embed in enumerate(embeddings):\n",
    "\n",
    "        # create the axes\n",
    "        ax = fig.add_subplot(gs[i // n_columns, i%n_columns], projection='3d')\n",
    "\n",
    "        ax.set_xlabel(\"latent 1\", labelpad=0.001, fontsize=13)\n",
    "        ax.set_ylabel(\"latent 2\", labelpad=0.001, fontsize=13)\n",
    "        ax.set_zlabel(\"latent 3\", labelpad=0.001, fontsize=13)\n",
    "\n",
    "        # Hide X and Y axes label marks\n",
    "        ax.xaxis.set_tick_params(labelbottom=False)\n",
    "        ax.yaxis.set_tick_params(labelleft=False)\n",
    "        ax.zaxis.set_tick_params(labelright=False)\n",
    "\n",
    "        # Hide X and Y axes tick marks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_zticks([])\n",
    "\n",
    "\n",
    "        aucs, errors = np.round(cp.get_auc(embeddings, labels),3)\n",
    "        titles=['DA only, AUC:{}'.format(aucs[0]),'NE only, AUC:{}'.format(aucs[1]), '5HT only, AUC:{}'.format(aucs[2]), 'ACh only, AUC:{}'.format(aucs[3]), f'All nms, AUC:{aucs[4]}']\n",
    "\n",
    "\n",
    "        # plot the embedding\n",
    "        cebra.plot_embedding(embedding=embed[l_class[0],:], embedding_labels=labels[l_class[0]], ax=ax, markersize=2,title=titles[i], cmap=c[0])\n",
    "        cebra.plot_embedding(embedding=embed[l_class[1],:], embedding_labels=labels[l_class[1]], ax=ax, markersize=2,title=titles[i], cmap=c[1])\n",
    "\n",
    "    plt.suptitle(t, fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(behaviour_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "plot4_embeddings(behaviour_embeddings,reward_labels, [rewarded, unrewarded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Predict one NM from an embedding of another NM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_nm = behaviour_embeddings[0]\n",
    "input_nm = behaviour_embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, prediction = cp.reconstruction_score(input_nm, target_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.471"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = np.round(score,3)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(target_nm, prediction, labels=reward_labels, label_classes=[rewarded, unrewarded], scores=[1,score], titles=['target nm', 'prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you play around with the input and target nms, (the order is: DA, NE, 5HT, ACh), it seems that it is not easy to predict the activity of one NM from another one (all reconstruction scores are below 0.5). However, it is much easier to reconstruct DA and NE from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3435324 ,  4.9708314 ,  5.597938  , ...,  0.13649794,\n",
       "       -0.10833851, -0.10031597], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviour_embeddings[0][:,2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
